<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.318">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Oregon Extended Assessment Technical Documentation - 2&nbsp; Assessment Operations</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<link href="./ce3.html" rel="next">
<link href="./ce1.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link id="quarto-text-highlighting-styles" href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


</head>

<body class="floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Assessment Operations</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Oregon Extended Assessment Technical Documentation</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/UO-BRT/orext-techreport-template" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="./Oregon-Extended-Assessment-Technical-Documentation.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Introduction</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ce1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statewide System</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ce2.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Assessment Operations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ce3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Technical Quality: Validity</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ce4.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Technical Quality: Other</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ce5.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Inclusion of All Students</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ce6.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Standards and Reporting</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Appendices</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Statewide System of Standards and Assessments</span></a>
  </div>
</li>
    </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#test-design-and-development" id="toc-test-design-and-development" class="nav-link active" data-scroll-target="#test-design-and-development"> <span class="header-section-number">2.1</span> 2.1 Test Design and Development</a>
  <ul class="collapse">
  <li><a href="#a-orext-purpose" id="toc-a-orext-purpose" class="nav-link" data-scroll-target="#a-orext-purpose"> <span class="header-section-number">2.1.1</span> 2.1A ORExt Purpose</a></li>
  <li><a href="#b-orext-test-blueprint" id="toc-b-orext-test-blueprint" class="nav-link" data-scroll-target="#b-orext-test-blueprint"> <span class="header-section-number">2.1.2</span> 2.1B ORExt Test Blueprint</a></li>
  <li><a href="#c-test-development-processes" id="toc-c-test-development-processes" class="nav-link" data-scroll-target="#c-test-development-processes"> <span class="header-section-number">2.1.3</span> 2.1C Test Development Processes</a></li>
  <li><a href="#d-computer-adaptive-considerations" id="toc-d-computer-adaptive-considerations" class="nav-link" data-scroll-target="#d-computer-adaptive-considerations"> <span class="header-section-number">2.1.4</span> 2.1D Computer-Adaptive Considerations</a></li>
  </ul></li>
  <li><a href="#item-development" id="toc-item-development" class="nav-link" data-scroll-target="#item-development"> <span class="header-section-number">2.2</span> 2.2 Item Development</a>
  <ul class="collapse">
  <li><a href="#project-description" id="toc-project-description" class="nav-link" data-scroll-target="#project-description"> <span class="header-section-number">2.2.1</span> Project Description:</a></li>
  <li><a href="#minimum-qualifications" id="toc-minimum-qualifications" class="nav-link" data-scroll-target="#minimum-qualifications"> <span class="header-section-number">2.2.2</span> Minimum Qualifications:</a></li>
  <li><a href="#compensation" id="toc-compensation" class="nav-link" data-scroll-target="#compensation"> <span class="header-section-number">2.2.3</span> Compensation:</a></li>
  <li><a href="#contact" id="toc-contact" class="nav-link" data-scroll-target="#contact"> <span class="header-section-number">2.2.4</span> Contact:</a></li>
  </ul></li>
  <li><a href="#test-administration" id="toc-test-administration" class="nav-link" data-scroll-target="#test-administration"> <span class="header-section-number">2.3</span> 2.3 Test Administration</a>
  <ul class="collapse">
  <li><a href="#a-administration-and-accommodations" id="toc-a-administration-and-accommodations" class="nav-link" data-scroll-target="#a-administration-and-accommodations"> <span class="header-section-number">2.3.1</span> 2.3A Administration and Accommodations</a></li>
  <li><a href="#b-comprehensive-training-system" id="toc-b-comprehensive-training-system" class="nav-link" data-scroll-target="#b-comprehensive-training-system"> <span class="header-section-number">2.3.2</span> 2.3B Comprehensive Training System</a></li>
  <li><a href="#c-technology-based-assessments" id="toc-c-technology-based-assessments" class="nav-link" data-scroll-target="#c-technology-based-assessments"> <span class="header-section-number">2.3.3</span> 2.3C Technology-based Assessments</a></li>
  </ul></li>
  <li><a href="#monitoring-test-administration" id="toc-monitoring-test-administration" class="nav-link" data-scroll-target="#monitoring-test-administration"> <span class="header-section-number">2.4</span> 2.4 Monitoring Test Administration</a></li>
  <li><a href="#test-security" id="toc-test-security" class="nav-link" data-scroll-target="#test-security"> <span class="header-section-number">2.5</span> 2.5 Test Security</a>
  <ul class="collapse">
  <li><a href="#a-prevention-of-assessment-irregularities" id="toc-a-prevention-of-assessment-irregularities" class="nav-link" data-scroll-target="#a-prevention-of-assessment-irregularities"> <span class="header-section-number">2.5.1</span> 2.5A Prevention of Assessment Irregularities</a></li>
  <li><a href="#b-detection-of-test-irregularities" id="toc-b-detection-of-test-irregularities" class="nav-link" data-scroll-target="#b-detection-of-test-irregularities"> <span class="header-section-number">2.5.2</span> 2.5B Detection of Test Irregularities</a></li>
  <li><a href="#c-remediation-following-test-security-incidents" id="toc-c-remediation-following-test-security-incidents" class="nav-link" data-scroll-target="#c-remediation-following-test-security-incidents"> <span class="header-section-number">2.5.3</span> 2.5C Remediation Following Test Security Incidents</a></li>
  <li><a href="#d-investigation-of-test-irregularities" id="toc-d-investigation-of-test-irregularities" class="nav-link" data-scroll-target="#d-investigation-of-test-irregularities"> <span class="header-section-number">2.5.4</span> 2.5D Investigation of Test Irregularities</a></li>
  </ul></li>
  <li><a href="#systems-for-protecting-data-integrity-and-privacy" id="toc-systems-for-protecting-data-integrity-and-privacy" class="nav-link" data-scroll-target="#systems-for-protecting-data-integrity-and-privacy"> <span class="header-section-number">2.6</span> 2.6 Systems for Protecting Data Integrity and Privacy</a>
  <ul class="collapse">
  <li><a href="#a-integrity-of-test-materials" id="toc-a-integrity-of-test-materials" class="nav-link" data-scroll-target="#a-integrity-of-test-materials"> <span class="header-section-number">2.6.1</span> 2.6A Integrity of Test Materials</a></li>
  <li><a href="#b-secure-student-level-assessment-data" id="toc-b-secure-student-level-assessment-data" class="nav-link" data-scroll-target="#b-secure-student-level-assessment-data"> <span class="header-section-number">2.6.2</span> 2.6B Secure Student-Level Assessment Data</a></li>
  <li><a href="#c-protecting-personally-identifiable-information" id="toc-c-protecting-personally-identifiable-information" class="nav-link" data-scroll-target="#c-protecting-personally-identifiable-information"> <span class="header-section-number">2.6.3</span> 2.6C Protecting Personally Identifiable Information</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Assessment Operations</span></h1>
</div>





<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="test-design-and-development" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="test-design-and-development"><span class="header-section-number">2.1</span> 2.1 Test Design and Development</h2>
<p>The test specifications document that describes our approach to assessment and test design for the ORExt is published in <em>Appendix</em> 2.1. The document includes our approach to reducing the depth, breadth, and complexity (RDBC) of grade level content standards, an overview of the essentialization process and EAF documents, the planned test design for the ORExt, test development considerations, sample test items, item specifications, and universal tools/designated supports/accommodations. Only Grade 7 Math field test items were developed in 2017-18 which were in accordance with the 2014-15 test specifications, and are the most current available.</p>
<section id="a-orext-purpose" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="a-orext-purpose"><span class="header-section-number">2.1.1</span> 2.1A ORExt Purpose</h3>
<p>The stated purpose of the ORExt is to provide the state technically adequate student performance data to ascertain proficiency on grade level state content standards for students with significant cognitive disabilities. A long-term goal of the program is to also provide information regarding annual student growth related to these content standards over Grades 3-8, as measured by vertically scaled assessments in ELA and Mathematics. The results of the assessment are currently reported in comparison to four performance levels: Level 1, Level 2, Level 3, and Level 4. Levels 3 and 4 denote a proficient level of performance, while Levels 1 and 2 denote performance that is not proficient. BRT and ODE developed a scaled score interpretation guide to assist stakeholders in interpreting the meaning of the scaled scores generated by the ORExt, supported by the state’s achievement level descriptors. This guidance is published in <em>Appendix</em> 2.1A.</p>
</section>
<section id="b-orext-test-blueprint" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2" class="anchored" data-anchor-id="b-orext-test-blueprint"><span class="header-section-number">2.1.2</span> 2.1B ORExt Test Blueprint</h3>
<p><em>Appendix</em> 2.1B includes the entire test blueprint for the ORExt, as conveyed by the balance of representation across content areas and domains. Field-testing is conducted each year in order to support the continuous improvement of test functioning. However, items are selected to maintain this balance of representation. Oregon teachers validated the content of the assessment, agreeing with the standards that were and were not selected to develop the Essentialized Standards to which the ORExt test items are aligned.</p>
</section>
<section id="c-test-development-processes" class="level3" data-number="2.1.3">
<h3 data-number="2.1.3" class="anchored" data-anchor-id="c-test-development-processes"><span class="header-section-number">2.1.3</span> 2.1C Test Development Processes</h3>
<p>The test development process implemented for the ORExt is conveyed in <em>Appendix</em> 2.1C, including standard selection and validation, item development, item review, review of all Oregon teacher feedback and updating of items, and scaling and item selection. The <em>Appendix</em> articulates the process used to generate the materials with comma separated value files used to create item templates that fed into Adobe InDesign© through a data merge. Final test packages are reviewed for accuracy and content and then disseminated via secure file transfer to Oregon Qualified Assessors.</p>
</section>
<section id="d-computer-adaptive-considerations" class="level3" data-number="2.1.4">
<h3 data-number="2.1.4" class="anchored" data-anchor-id="d-computer-adaptive-considerations"><span class="header-section-number">2.1.4</span> 2.1D Computer-Adaptive Considerations</h3>
<p>The ORExt is not a computer-adaptive instrument, so these concerns do not apply.</p>
</section>
</section>
<section id="item-development" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="item-development"><span class="header-section-number">2.2</span> 2.2 Item Development</h2>
<p>Item writers were recruited by ODE staff using an existing Qualified Assessor/Qualified Trainer listserv.</p>
<p><img src="img/ItemDev.png" class="img-fluid"></p>
<section id="project-description" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="project-description"><span class="header-section-number">2.2.1</span> Project Description:</h3>
<p>Behavioral Research and Teaching at the University of Oregon recruited Oregon teachers to participate in item development for a new alternate assessment. Selected teachers were asked to develop 360 items in English Language Arts, Mathematics, or Science over the course of the summer, from mid-June through end of August. The Project Director worked with lead item developers to provide training, ongoing review and feedback, and quality assurance. All participants were expected to provide documentation of their qualifications and sign test security agreements. In addition, all item developers were expected to participate in a half-day item development training based upon the following schedule: ELA - Tuesday, from 8 AM to 12 PM; Math - Wednesday, from 8 AM to 12 PM; Science - Thursday, from 8 AM to 12 PM.</p>
</section>
<section id="minimum-qualifications" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="minimum-qualifications"><span class="header-section-number">2.2.2</span> Minimum Qualifications:</h3>
<p>All licensed Oregon public school teachers with at least three years of teaching in a life skills/severe needs program (SPED) or a general education classroom (GEN-ED), respectively, were encouraged to apply. Preference was given for item writing experience, additional years of teaching experience, and higher education degree status.</p>
</section>
<section id="compensation" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="compensation"><span class="header-section-number">2.2.3</span> Compensation:</h3>
<p>Teachers who participated in this process were compensated at a rate of $20/hr via professional service contracts. It was anticipated that teachers would produce 4 ELA items/hr, 6 Science items/hr, and 8 Math items/hr. As such, the maximum contract amount for ELA was $1,800, for Science $1,440, and for Math $900. Item development focused primarily on writing the stem and 3 options, with no need to produce graphics (rather use labels for a BRT graphic designer to produce).</p>
</section>
<section id="contact" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="contact"><span class="header-section-number">2.2.4</span> Contact:</h3>
<p>Because the timeline required work over the summer, Oregon teacher recruitment was challenging. BRT researchers thus performed an additional on-campus recruitment within the College of Education using the same information. The final pool of item writers included 18 item writers: seven Oregon teachers (all with MA degrees), five PhD candidates within the COE, and six BRT researchers (four PhD candidates, one PhD, and one with an MA). Item writers averaged 11.5 years of teaching experience. The teachers recruited all had prior experience developing items for the ORExt, as did all of the BRT researchers. The five PhD candidates within the COE had no prior item development experience. All item development was reviewed by BRT researchers and the Project Manager.</p>
<p>The item development process followed is elaborated in <em>Appendix</em> 2.2.1, which is the PowerPoint used in training all Oregon item writers. The item development process was structured with the following steps. Item writers were first oriented to the student population, as the pool of item writers included both content and special education experts. The Essentialization Process used to RDBC grade level standards was then modeled so writers would understand how the item alignment targets, the Essentialized Standards, were generated. Lecture, guided practice, and independent practice activities and follow-up discussion ensured comprehension of the process. BRT staff developed exemplar items for every Essentialized Standard, varying the complexity from Low (L) to Medium (M) to High (H) levels of complexity to convey the different performance expectations at each level. The balanced vertical scaling design provided an overall form-to-form and grade-to-grade level framework for the test formation process once items were developed (see <em>Appendix</em> 2.2.2). Sample items are provided in <em>Appendix</em> 2.2.3 for stakeholder reference, demonstrating the format and style of typical items on the ORExt.</p>
</section>
</section>
<section id="test-administration" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="test-administration"><span class="header-section-number">2.3</span> 2.3 Test Administration</h2>
<p>The ORExt assessments are administered according to the administration, scoring, analysis, and reporting criteria established in the ORExt General Administration Manual (see <em>Appendix</em> 2.3). Important updates to the testing process are distributed via the <a href="http://www.oregon.gov/ode/educator-resources/assessment/Pages/Assessment-and-Accountability-Update.aspx">Assessment and Accountability Updates</a> listserve, as well. ODE uses this system to communicate information that is relevant for the statewide assessment system, including the ORExt. Announcements are sent to the listserv by email and are also posted to the ODE website. The standardization of test administration is supported by a comprehensive training process described below in Section 2.3B.</p>
<section id="a-administration-and-accommodations" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="a-administration-and-accommodations"><span class="header-section-number">2.3.1</span> 2.3A Administration and Accommodations</h3>
<p>The state has ensured that appropriate universal tools, designated supports, and accommodations are available to students with disabilities and students covered by Section 504 by providing guidance and technical support on accommodations (see <em>Appendices</em> 2.3A.1 and 2.3A.2). Guidelines regarding use of the accommodations for instructional purposes are included in the document, as all students are expected to receive test accommodations that are consistent with instructional accommodations.</p>
<p>Accommodations are built into the flexibility provided by the ORExt test though they have not yet been researched for the ORExt. However, annual training and proficiency testing efforts related to becoming a qualified assessor and/or qualified trainer for the ORExt support standardized use of available accommodations that are not already part of the test design. Based on annual analyses, results demonstrate that student performance varies according to their abilities and not construct-irrelevant factors, such as sex, race, or ethnicity (See Section 4.2).</p>
<p>The state has ensured that appropriate accommodations are available to students with limited English proficiency by providing guidance and technical support on accommodations (see <em>Appendix</em> 2.3A.1). Communication systems for this student population are limited; exposure to multiple languages can make a student’s communication system more complex. The ORExt uses universal design principles and simplified language approaches in order to increase language access to test content for all students. In addition, directions and prompts may be translated/interpreted for students in their native language.</p>
<p>An analysis of accommodated versus non-accommodated administrations is needed in order to demonstrate that the provision of language accommodations is not providing any advantage to students with limited English proficiency, nor any disadvantage to other participants. Accommodations information was collected this year as an option for data entry. Entering accommodations information will be required each year. Analyses of the impact of accommodation provision on the ORExt is feasible after each years administration.</p>
<p>The Oregon Extended assessments can be administered using both Large Print and Braille (contracted and non-contracted) versions, as well. Oregon has ensured that the Oregon Extended assessments provide an appropriate variety of accommodations for students with disabilities. The state has provided guidance on accommodations in presentation, response, setting, and timing in the Accessibility Manual 2021-22: How to Select, Administer, and Evaluate Accommodations for Oregon’s Statewide Assessments (see <em>Appendix</em> 2.3A.2). The Oregon Extended assessments are also designed according to universal design principles and utilize a simplified language approach (see <em>Appendix</em> 2.3A.3).</p>
<p>In the 2021-2022 school year, the state redesigned a training and proficiency program for sign language interpretation of its assessments and has significantly updated the site during this time. The <a href="http://or.k12test.com">sign language training</a> process included videos of interpreters administering items to students, materials that support appropriate administration (i.e., transcripts, closed captioning and PowerPoint slides that supplement the video administrations and the current ODE accessibility manual), and proficiency testing to support standardized interpretation for Oregon’s assessments, including the ORExt. A 10-item proficiency test was administered, with an 80% required for passing (8/10 items correct). In 2021-22, the site was used to train 48 participants. All participants passed the assessment on the first attempt. The overall average score on the proficiency test was 95.9%.</p>
<p>The ORExt assessments provide an appropriate variety of linguistic accommodations for students with limited English proficiency. They also use a simplified language approach in test development in order to reduce language load of all items systematically (see <em>Appendix</em> 2.3A3). Any given student’s communication system may include home signs, school signs, English words, and Spanish words, for example. With the exception of items that require independent reading, the ORExt assessment can be translated or interpreted by a Qualified Assessor (QA) working with an interpreter in the student’s native language, including American Sign Language. QAs are allowed to translate/interpret the test directions. QAs can adapt the assessment to meet the needs of the student, while still maintaining standardization due to systematic prompts and well-defined answers.</p>
</section>
<section id="b-comprehensive-training-system" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="b-comprehensive-training-system"><span class="header-section-number">2.3.2</span> 2.3B Comprehensive Training System</h3>
<p>Comprehensive information for ongoing training for all qualified assessors (QAs) and Qualified Trainers (QTs) is provided in <em>Appendices</em> 2.3B.1-2.3B.8. Through an online distribution and assessment system, <a href="https://or.k12test.com/">QA/QT Training and Proficiency</a> is determined anually. This website hosts all resources and information needed to administer, score, report, and interpret the results from the ORExt. The website also includes proficiency assessments that are required for all QAs and QTs who may administer the ORExt. QTs are directly trained by ODE and BRT staff as part of a train the trainers model. QTs then provide direct trainings for new QAs in their respective regions.</p>
<p>The Oregon Department of Education (ODE) provided four direct statewide trainings for new Qualified Trainers (QTs) and returning QTs in Zoom regional trainings. The the regional trainings were provided at two separate time slots on November 10th and November 17 for a total of four QT training opportunities.</p>
<p>Only trained Qualified Assessors (QAs) can administer the Oregon Extended assessment. Qualified Assessors who also receive direct instruction from ODE and BRT may become Qualified Trainers (QTs) who are certified to train local staff using the train-the-trainers model. Training for new assessors must be completed on an annual basis. Assessors who do not maintain their respective certifications for any given year must re-train if they choose to enter the system again.</p>
<p>The tables below contain data from the <a href="http://or.k12test.com/">Oregon Extended Assessment Training and Proficiency Website</a>. All assessors need to complete some form of training each year to retain their status for administering the Extended Assessments.</p>
<p>New assessors and returning assessors needed further training in 2021-22 and were required to pass proficiencies with a score of 80% or higher. These proficiencies covered areas in Administration, English Language Arts (ELA), Mathematics, and Science. Returning QAs or QTs for the 2021-22 school year only needed to pass a Refresher Proficiency, again with a score of 80% or higher. The tables below contain data on the number of assessors (participants) in each of the proficiencies, as well as the Refresher Proficiency. Included in the data is the number of attempts needed to attain a passing score as well as the average passing score of the participants.</p>
<p>An analysis of the Oregon Extended Assessment Training and Proficiency Website showed #? Assessors in-Training, #? Qualified Assessors, and #? Qualified Trainers registered with the system.</p>
<p><img src="img/QA_QT_TestPartic_1819.png" class="img-fluid"></p>
<p>A higher number of assessors completed the Refresher Proficiency test than the subject area proficiency tests reflecting a greater number of return assessors compared to new assessors. Data showed ELA Proficiency to be the most challenging to new assessors, but most were able to pass on the first or second attempt with about 1% or less of assessors requiring more than two attempts and science to be the least challenging with 100% of assessors passing on the first attempt. The majority of assessors passed the Administration and Math proficiency tests on the first and second attempts with less than 1% requiring a third attempt. The majority of returning assessors passed the refresher on the first attempt with less than 2% requiring a seond and third attempt. There were 12 fewer Qualified Assessors and 2 more Qualified Trainers compared to last year.</p>
<p>Evaluations are collected at each QT training in November. The results reflect general approval, but also suggest areas of improvement that ODE and BRT work on for subsequent trainings/subsequent years, as appropriate. QT evaluations this year included positively worded statements regarding the quality of training rated on a scale where 1 = Strongly Disagree, 2 = Disagree, 3 = Agree, and 4 = Strongly Agree.</p>
<p>The first section evaluated the state-level information and the knowledge of the ODE presenters, the participants’ level of comfort with the training provided, the participants’ ability to carry this training and materials back to train district staff, and the overall utility of the training. Seventy-six percent of participants strongly agreed with these statements, 24% agreed, and less than 3% disagreed and strongly disagreed, collectively. In the second section, participants were asked to evaluate the BRT trainers and their guidelines regarding how to use the training and proficiency website and related resources. Seventy-nine percent of participants strongly agreed with these statements, 19% agreed, and less than 2% disagreed and strongly disagreed, collectively. Overall, these results demonstrate that participants felt that the training was high quality and they felt confident that they could train their staff upon return to their respective districts with the knowledge and resources gained. This year’s QT training cycle included an optional afternoon session for any interested educators on how to essentialize grade level content standards and how to develop curriculum and provide instruction that is aligned to those standards for students who are functioning off grade level, with a focus on students with significant cognitive disabilities (SWSCD). We asked participants to rate their confidence in using the knowledge acquired during the session as well as to evaluate the quality of the presentation and materials. A four-point scale was employed (Strongly Disagree, Disagree, Agree, Strongly Agree). Percentages of responses for each statement used in the survey are provided below. The table provides a summary of the data related to participant confidence and their evaluation of the quality of the presentation. The respondent n-sizes ranged from 26-30, depending upon the question.</p>
<p>Note: Results are very positive, with some reviewers feeling less confident about their abilities to train others about the essentialization process. This outcome was expected. The process is complex, particularly given the understanding that this was the first time they had received such training.</p>
<p><img src="img/TrainingConfidenceScale.png" class="img-fluid"></p>
<p>In addition, all technical assistance questions that we receive from the field as part of our HelpDesk are documented. The most common inquiries for the 2018-19 test administration window involved credential verification to add schools/students to rosters, selecting appropriate IDEA disability codes for rosters, and manual grading for the electronic ELA writing items. Some other common inquiries included exit PIN, access to monitoring for DTC’s, and technical issues with individual tablet. All helpdesk inquiries will be taken into consideration as the training site is updated for the 2019-2020 testing year.</p>
<p>The <em>HelpDesk</em> log is published in <em>Appendix</em> 2.3B.9.</p>
<p>Oregon monitors the quality of its system in several ways in order to support continuous improvement. In terms of the assessment quality, item statistics are reviewed each year and items that are not functioning as intended are removed and replaced by better functioning field-test items.</p>
<p>In 2014-15, items were reviewed in two phases, first using classical test theory (CTT) and second using Rasch analyses. All items flagged as a result of the statistical reviews were analyzed, item-by-item, by a team of measurement and content experts at BRT. Not all flagged items were removed, as several did not have apparent design flaws. Considerations regarding domain representation as well as item difficulty range also were considered during the review process. We also employed different decision rules for unique items versus horizontally- or vertically-scaled anchor items. It was important in many cases to maintain anchor items. Items with clear design flaws were removed from subsequent analyses and reporting. The following flagging criteria were employed:</p>
<ul>
<li><strong>CTT</strong>: A unique item was flagged if it had a p-value of .10 or lower, .90 or higher, or a point biserial &lt; .15. Anchor items were flagged if they had a p-value of .10 or lower or .95 and higher on all forms or a point biserial &lt; .45 on any form.</li>
<li><strong>Rasch</strong>: Unique items were flagged if their outfit mean square values were between 0 and .25 or &gt; 1.5. Anchor items were flagged if their outfit mean square values were &lt; .5, &gt; 1.8 for horizontal items, or &gt; 2.0 for vertical anchor items.</li>
</ul>
<p>Out of a total of 5,929 items developed in 2014-15, 166 were removed (2.8%).</p>
<p>We also implement a consequential validity study each year that surveys QAs and QTs regarding the academic and social consequences of the ORExt, both intended and unintended. The Consequential Validity report is published in <em>Appendix</em> 2.3B.10. ODE and BRT staff review the results of the survey annually to determine what program improvements are needed. A summary of the results is provided below.</p>
<p>ODE implemented a research survey program to address the need to document the consequences, both intended and unintended, of the ORExt Assessments. The research questions have been framed based upon current consequential validity approaches for alternate assessments in the literature, as well as issues that are of specific value in Oregon. The survey included 121 respondents. This was 11% of the solicited respondents, who were all Qualified Assessors (QAs) and Qualified Trainers (QTs) in the or.k12test.com database. The sample was 83% female and represented all regions of the state, as well as age ranges. The survey included a range of quantitative and qualitative components. The quantitative results demonstrate that QAs and QTs continue to feel that the ORExt test items were easy to administer and score (64.2% Strongly Agree) and felt confident in their ability to interpret scaled scores and Achievement Level Descriptors for the ORExt (69.8% Strongly Agree and Agree). They also felt that the items were accessible for students who participated (78% Strongly Agree and Agree) and that the ORExt reflected the academic content that SWSCD should be learning (68.4% Strongly Agree and Agree). QAs and QTs felt marginally positive about the educational impacts of the ORExt and marginally negative about its social impacts. The results again demonstrate that the ORExt content area assessments generally require up to one hour to administer.</p>
<p>The qualitative results revealed two areas in which educators appreciated the ORExt and four areas of needed improvement. QAs and QTs said that they appreciated: 1) the assessment’s efficiency (i.e., more streamlined administration, ease of administration, easier to give and score online, online materials distribution); and, 2) overall item and test design (i.e., one item per page, visual supports, scoring protocol and student materials design, accessibility of test questions). Teachers recommended the following areas of improvement, not all of which are actionable: 1) Option to administer the assessment electronically, 2) A functional skills assessment, 3) New items for very low functioning students should be developed, and 4) A math assessment composed of more practical/life skills problems involving time and money. Complete results, including anticipated responses, from the survey can be found in <em>Appendix</em> 2.3B.10.</p>
</section>
<section id="c-technology-based-assessments" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="c-technology-based-assessments"><span class="header-section-number">2.3.3</span> 2.3C Technology-based Assessments</h3>
<p>The ORExt was implemented using a technology-based platform as Phase 3 of the ORExt Tablet Administration. The 2017-18 testing window was the first year all grade level and subject area assessments were available on a tablet application/web-based platform. Administration of the tablet application mirrors paper/pencil administration with each item read aloud to the student, and the student asked to select one of three answer choices. Tablet functionality includes optional discontinuation if the student misses 10 out of the first 15 items, directing the assessor to administer the ORora. To support understanding of the system by both teachers and students, a separate practice test tablet application is available. Helpdesk inquiries and feedback from the field indicated much preference of the tablet administration versus paper/pencil. Qualified Trainers and Qualified Assessors reported their students’ were more focused during tablet administration, and because the tablet application scores automatically it was much more efficient for assessors. Improvements will be made to the electronic test based on technology improvements and feedback from the field. Data entry for all platforms is now maintained and monitored by secure BRT servers.</p>
</section>
</section>
<section id="monitoring-test-administration" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="monitoring-test-administration"><span class="header-section-number">2.4</span> 2.4 Monitoring Test Administration</h2>
<p>The ODE maintains a rigorous training system to support standardized test administration for the [Oregon K12 website](https://or.k12test.com, (secure website, but see screenshot below for an example of training content).</p>
<p><img src="img/TrainingSiteContentExample.png" class="img-fluid"></p>
<p>The or.k12test.com website includes a training section that addresses any systems updates, the process for becoming a Qualified Assessor or Qualified Trainer, student eligibility expectations, student confidentiality and test security, test administration and scoring expectations, examples of appropriate and inappropriate administration (video), supporting student access to items without violating the test construct, content area trainings that demonstrate how to administer items in ELA, Math, and Science (video, with supporting test materials), and how to access secure tests and complete data entry. Information for QAs, QTs, and parents regarding the ORExt is also provided, as are all necessary support materials. For QAs, these materials include practice tests to prepare both themselves and students for the annual assessment and all of the training materials used on the website. In addition to these materials, QTs have access to all training materials necessary to provide annual training to QAs in their purview (see screenshot below):</p>
<p><img src="img/MaterialsToDownload.png" class="img-fluid"></p>
<p>In addition, monitoring and reporting related to test administration issues for the ORExt is addressed via general ODE reporting systems. Information regarding this process can be located in the general assessment system Peer Review evidence submission.</p>
</section>
<section id="test-security" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="test-security"><span class="header-section-number">2.5</span> 2.5 Test Security</h2>
<section id="a-prevention-of-assessment-irregularities" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="a-prevention-of-assessment-irregularities"><span class="header-section-number">2.5.1</span> 2.5A Prevention of Assessment Irregularities</h3>
<p>Test security policies and consequences for violation are addressed in the Test Administration Manual on an annual basis (see <em>Appendix</em> 1.4.2, p.&nbsp;29-33). These policies include test material security, proper test preparation guidelines and administration procedures, consequences for confirmed violations of test security, and annual training requirements at the district and school levels for all individuals involved in test administration. Consequences for adult-initiated test irregularities may be severe, including placing teaching licenses in jeopardy (see <em>Appendix</em> 1.4.2, p.&nbsp;31-33).</p>
</section>
<section id="b-detection-of-test-irregularities" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="b-detection-of-test-irregularities"><span class="header-section-number">2.5.2</span> 2.5B Detection of Test Irregularities</h3>
<p>The ODE utilizes a localized monitoring system where school test coordinators oversee building-level administration by trained, Qualified Assessors, and report to centralized district test coordinators, who are then responsible for reporting any confirmed violations to ODE. Improprieties are defined as adult-initiated or student-initiated and investigated accordingly (see <em>Appendix</em> 1.4.2, p.&nbsp;29-31).</p>
</section>
<section id="c-remediation-following-test-security-incidents" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="c-remediation-following-test-security-incidents"><span class="header-section-number">2.5.3</span> 2.5C Remediation Following Test Security Incidents</h3>
<p>ODE’s alternate assessment program manager investigates and remediates substantiated test security incidents for the ORExt by working with district test coordinators. Additional information regarding this process can be located in the general assessment system Peer Review evidence submission.</p>
</section>
<section id="d-investigation-of-test-irregularities" class="level3" data-number="2.5.4">
<h3 data-number="2.5.4" class="anchored" data-anchor-id="d-investigation-of-test-irregularities"><span class="header-section-number">2.5.4</span> 2.5D Investigation of Test Irregularities</h3>
<p>School and district test coordinators conduct initial investigations into all alleged test irregularities. Once reported to ODE, all alleged test irregularities are investigated in consultation with district test coordinators and the test vendor, as appropriate (see <em>Appendix</em> 1.4.2, p.&nbsp;31-33). In the event that a test irregularity is determined to be factual, consequences are determined based upon contextual issues that are brought to light during the investigation. Additional information regarding this process can be located in the general assessment system Peer Review evidence submission.</p>
</section>
</section>
<section id="systems-for-protecting-data-integrity-and-privacy" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="systems-for-protecting-data-integrity-and-privacy"><span class="header-section-number">2.6</span> 2.6 Systems for Protecting Data Integrity and Privacy</h2>
<section id="a-integrity-of-test-materials" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="a-integrity-of-test-materials"><span class="header-section-number">2.6.1</span> 2.6A Integrity of Test Materials</h3>
<p>Test materials for the ORExt are maintained throughout development, dissemination, and administration via multiple mechanisms. All items under development are stored in secure file servers managed by Behavioral Research &amp; Teaching at the University of Oregon, the test vendor for the ORExt. Item reviews necessary to provide alignment, bias, and sensitivity information are conducted online using the secure <a href="http://brtitemreview.com">Distributed Item Review (DIR)</a> platform (secure website, but see <em>Appendix</em> 3.1B for a system overview).</p>
<p>For the 2018-2019 school year, all paper/pencil secure test distribution and data entry was hosted by BRT through the secure training site.</p>
<p>The secure tablet application and web-based platform distribution and data entry were hosted by BRT servers. All technology based secure administration and data entry was password-protected. Download of the tablet app was dependent on the type of device, all instructions and download links were available in the Test App User Guide (see <em>Appendix</em> 2.6A) Additional information regarding test security can be located in the general assessment system Peer Review evidence submission.</p>
<p><img src="img/DataEntryPg.png" class="img-fluid"></p>
</section>
<section id="b-secure-student-level-assessment-data" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="b-secure-student-level-assessment-data"><span class="header-section-number">2.6.2</span> 2.6B Secure Student-Level Assessment Data</h3>
<p>Student level datais protected by relevant training and through a secure data system in which all data entry is conducted online using password-protected, secure procedures on the <a href="https://or.k12test.com">Oregon K12 website</a> Only trained users with a vested educational interest who have signed test security agreements are authorized to access to online data entry systems.</p>
</section>
<section id="c-protecting-personally-identifiable-information" class="level3" data-number="2.6.3">
<h3 data-number="2.6.3" class="anchored" data-anchor-id="c-protecting-personally-identifiable-information"><span class="header-section-number">2.6.3</span> 2.6C Protecting Personally Identifiable Information</h3>
<p>All confidential, personally identifiable student information is protected by policy and supported by training (see <em>Appendix</em> 1.4.2, p.&nbsp;26). The minimum number of students necessary to allow reporting of students and student subgroups varies by rating (i.e., achievement, growth, graduation, and school size), by level (i.e., school/district/state), and by number of years of assessment data available. For example, to receive an achievement rating, schools must have at least 40 tests for the two most recent school years in reading or mathematics. Alternatively, small schools receive an achievement rating if they have at least 40 tests over the most recent four years. If a school does not have at least 40 tests over a four-year period, they will not receive an achievement score (see <em>Appendix</em> 2.6C). Similar rules are applied to student subgroups, including students with disabilities, English learners, and students from diverse racial/ethnic backgrounds (see <em>Appendix</em> 2.6C, p.&nbsp;7).</p>


</section>
</section>

</main> <!-- /main -->
<script type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./ce1.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Statewide System</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ce3.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Technical Quality: Validity</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>