# Technical Quality: Other

## 4.1 Reliability

Test reliability can be viewed through several lenses, all of which
document how consistently an assessment performs across occasions,
contexts, and raters. Typical strategies for addressing reliability
include documentation of internal consistency, split-half reliability,
and test-retest reliability. If multiple forms are implemented, test
form reliability documentation is also requisite. The implementation
plan for the ORExt includes initial documentation of internal
consistency (Cronbach's alpha). The 2015-16 technical report included
internal consistency estimates, split-half reliability analyses, as well
as a small test-retest assessment of reliability comparisons by means of
our pilot tablet administration study. There is only one test form for
the ORExt, so test form comparisons are not possible.

### 4.1A Test Reliability

Marginal reliability results (true score variance/true score variance +
error variance) demonstrate that the tests are quite reliable at the
total test level. Full reliability statistics for each of the
operational tests administered this year are provided below. These
results demonstrate that the total test reliabilities were quite high,
ranging from .67 to .91. Each table below provides the content area,
grade, and the marginal reliabilities. All test forms were composed of
36 operational and 12 embedded field-test items.

```{r}
# code for marginal reliability tables
```

### Test Information Functions

The test information functions published below also indicate that the
scales exhibit a reliability greater than or equal to .80 for all
proficient-level cutscores.

### English Language Arts TIFs

```{r}
# code for ELA TIFs
```

### Mathematics TIFs

```{r }
# code for Math TIFs
```

### Science TIFs

```{r}
# code for Science TIFs
```

### Validation of ORExt Vertical Scales

The Test Characteristic Curves (TCCs) for the grade-level assessments in
ELA and mathematics demonstrate incrementally increasing growth and test
demands across Grades 3-8, with the exception of Grade 7 mathematics.
The Grade 7 mathematics assessment was revised to be more difficult last
year, but clearly more elaboration of this effort is needed to address
its location on the TCC. Grade 11 and science tests are not vertically
scaled; TCCs are thus not presented for Grade 11 or science. All Rasch
model scaling, as well as the data visualizations for the TCCs were
conducted in the R software 3.3.2 environment (R Core Team, 2016) using
the r2Winsteps package (Anderson, 2015).

```{r}
# code for TCCs here
```

### 4.1B Overall and Conditional Standard Errors of Measure

The average SEM associated with each cut score for 2017-18 student data
are presented in the table below, supported by a KEY. The SEMs decreased
in almost all cases compared to last year, suggesting that the measures
are more reliable when student eligibility is more strictly controlled.
See Section 4.2 below for means and standard deviations by grade and
subject area. SEM = Standard Error of Measure associated with the cut
score to the left; averaged to the tenths' place. Level 1 = Does Not Yet
Meet (not included as the lowest level of proficiency) Level 2 = Nearly
Meets Level 3 = Meets Level 4 = Exceeds

```{r}
# code for tables with SE for each cut score
```

### 4.1C Classification Accuracy & Consistency

Results from the 2017-18 ORExt test administration were analyzed using
Rudner's classification index [@rudner05]. Results closer to 1.0
indicate the likelihood that a student was appropriately classified as
proficient or not proficient (accuracy) and the likelihood that the
student would be classified in the same category given an additional
test administration. The calculation utilizes item difficulty and theta
value distributions, as well as related standard errors of measurement,
to generate probabilistic estimates based on one test administration.
Complete results, generated from the cacIRT package in R, are provided
below. Results denote very high levels of classification accuracy and
consistency.

```{r class_acc}
# code for classification accuracy tables
```

The ORExt is not a computer-adaptive instrument so estimate precision
documentation based upon that test design is not provided.

## 4.2 Fairness and Accessibility

The state has taken steps to ensure fairness in the development of the
assessments, including an analysis of each test item by Oregon teachers
not only for linkage to standards, but also for access, sensitivity, and
bias (see *Appendix* 3.1A). In addition, we reviewed test functioning as
relevant to race/ethnicity and disability subgroups. This process
increases the likelihood that students are receiving instruction in
areas reflected in the assessment, and also that the items are not
biased toward a particular demographic or sub-group.

### Differential Item Functioning Analyses

To investigate Differential Item Functioning (DIF), the Mantel-Haenszel
test using a purification process was conducted [@holland88; @kamata04] 
with the R software using the difR package (Magis
et al., 2013). When using the Mantel-Haenszel test to investigate DIF,
contingency tables are constructed, and the resulting odds for the focal
group answering the item correctly are compared to the odds for the
reference group. Given n-size limitations (Scott, et al., 2009), we were
able to conduct two analyses: a) White/Non-White and b) Male/Female.
Whites and Males were the focal groups and Non-Whites and Females were
the reference groups, respectively. The contingency table summarizes
correct and incorrect responses to each item by respondents' total raw
score by subgroup (Kamata & Vaughn, 2004). If there is no difference in
performance for the two groups, the odds ratio of the focal group
performance to reference group performance will equal one. An odds ratio
greater than one means the focal group is performing better than the
reference group, with the opposite being true for odds ratios less than
one.

The difR package contains a built in algorithm to conduct purification
automatically, so we were interested in how this algorithm functioned
relative to the iterations conducted manually using SPSS. We used
criteria outlined by the Educational Testing Service (ETS) for DIF
Classification (Holland & Thayer, 1988) to determine whether or not
items exhibited DIF, as the difR package reports delta values by
default, defined as $$\Delta_{MH} =
-2.35*ln(\alpha_{MH})$$

The Holland and Thayer criteria were used for all Mantel-Haenszel
analyses. Items that were flagged as "C" level items were reviewed by
BRT researchers for potential biases. If biases are identified, the item
is removed from the item pool. DIF analyses were performed ex post facto
on the 2015-16 ORExt operational items to address longitudinal trends.
Only three ELA items were identified as exhibiting a "C" level DIF
across both 2017 and 2018. Those three ELA items, one in Grade 5 that
exhibited DIF that privileged White examinees, one in Grade 4 that
privileged Female examinees, and one in Grade 8 that privileged Female
examinees, were removed and were not used in 2017-18 or thereafter. DIF
analyses was also be performed in the 2017-18 school year to continue to
address DIF longitudinally. All items, including field test items, were
included in the analyses. There are a total of 48 items on each
assessment.

::: callout-note
## DIF Grades

-   A: $0 > \delta <= 1$
-   B: $1 > \delta <= 1.5$
-   C: $1.5 > \delta$
:::

Within the White/Non-White analysis, 10 out of 18 items flagged as "C"
level items privileged Non-White test participants in ELA, 2 out of 5
privileged Non-White test participants in Mathematics, and 2 out of 7
privileged Non-White test participants in Science. Overall, DIF flagging
bases on race was relatively balanced, with 14 privileging students who
were Non-White and 16 privileging students who were White.

```{r dif1}
# code for MH DIF tables here

extract_letter_grades <- function(dif_mod, items) {
  item_names <- names(items)
  delta  <- -2.35 * (log(dif_mod$alphaMH))
  grades <- symnum(
    abs(delta),
    c(0, 1, 1.5, Inf),
    symbols = c("A", "B", "C")
  )
  
  tidyr::tibble(item = item_names, delta, grades) %>% 
    dplyr::mutate(grades = as.character(grades))
}
```

In terms of the Male/Female analyses, 10 out of 16 items flagged as "C"
level items privileged Females in ELA, 4 out of 9 flagged items
privileged Females in Mathematics, and 8 out of 11 flagged items
privileged Females in Science. Overall, DIF flagging based on sex was
relatively balanced, with 22 privileging Females and 14 privileging
Males.

```{r }
# code to actually print gender DIF tables
```

### Race - Ethnicity Percentages and Totals by Content Area and Grade Level

The full ethnic and disability demographics for students taking the
ORExt are reported below. Students ethnicity/race was reported in seven
categories: (a) American Indian/Alaskan Native, (b) Asian, (c) Black or
African-American, (d) Multi-ethnic, (e) Native Hawaiian or Other Pacific
Islander, (f) Hispanic, or (g) White. The majority of students were
reported as White (53-68%) or Hispanic (12-27%). These results are
largely consistent with the demographics reported for the general
assessments, though percentages taking the ORExt are slightly higher for
most students of color and generally lower for students who are Asian or
White (see *Appendix* 4.2).

```{r}
# code for table of race/ethnicity proportions taking the test by grade/content
```

The majority of students who participated in the ORExt were students
with Intellectual Disability (30-45%) and students with Autism Spectrum
Disorder (28 -34%), followed by students with Other Health Impairment
(11-16%). ODE policy for 2015-16 changed to require students who
participate in the ORExt to take the assessment in all relevant content
areas. There is thus very little change in terms of participation
percentages across content areas, as evidenced by the total n-sizes per
grade level displayed below.

### Exceptionality Percentages By Content Area and Grade Level

```{r }
# code for participation by disability table 
# (same as above but by disability code)
```

### Observed Means and Standard Deviations

The following tables provide information regarding observed means and
standard deviations by content area and grade level. The Grade 3-8
English language arts and mathematics scaled scores are centered on 200,
while all Grade 11 scores are centered on 900 (to reinforce that they
are not on the vertical scale). Science is centered on 500 at Grade 5
and centered on 800 at Grade 8. The vertically scaled scores generally
convey incremental gains in achievement across grade levels, though the
results suggest small losses across grades in math.These scales were
selected to clearly determine whether scores are on the same scale and
also to differentiate among the statewide assessments in use to avoid
confusion (i.e., SBA, OAKS, ORExt, ELPA, KA). The general pattern is
that RIT scores decreased from 2014-15 to 2015-16. This decrease is
attributed not to the scale, nor to deceleration of growth, but to the
substantive shift in the tested student population as a result of ODE
eligibility guidelines. The scale from 2015-16 to 2016-17 appears to
have stabilized because the student population tested was more
consistent.

```{r}
# code to report observed means
```

#### Observed Means Reported by Sex

The following tables provide information regarding average student
performance by grade level and sex (Female/Male) in each of the content
areas assessed on the ORExt. Significant differences based on a Welch
two sample t-test are noted in Grades 5 and 12 in ELA, and Grade 8 in
mathematics.

```{r }
# code to report observed means by sex
```

#### Observed Means Reported by Race

The following table provides information regarding average student
performance by grade level and race/ethnicity in each of the content
areas assessed on the ORExt.

```{r}
# code to report observed means by race/ethnicity
```

#### Observed Means Reported by Exceptionality Status

The following table is a number key for **Elibibility Codes:**

##### Eligibility Codes List

-   0 Not Applicable
-   10 Intellectual Disability
-   20 Hearing Impairment
-   40 Vision Impairment
-   43 Deafblindness
-   50 Communication Disorder
-   60 Emotional Disturbance
-   70 Orthopedic Impairment
-   74 Traumatic Brain Injury
-   80 Other Health Impairment
-   82 Autism Spectrum Disorder
-   90 Specific Learning Disability

The following tables provide information regarding average student
performance by grade level and exceptionality category in each of the
content areas assessed on the ORExt. Students with SLD were generally
the highest performing group, though students with ED performed higher
at certain grade levels/content areas. The lowest performing group was
consistently students with VI.

```{r }
# code for means by disability code
```

#### Graphs of Observed Means By Disability

The graphs below convey information similar to that shared above in
graphic form. The graphics include 95% confidence interval error bars,
so determining which subgroups performed in a manner that is
significantly better than others is readily apparent by looking at the
location of the error bars. Error bars that do not overlap in terms of
the y-scale are significantly different. Students with VI are again the
lowest performing group. Students with SLD are consistently
outperforming most peers. Students with VI are consistently the lowest
performing group, which led to concerns regarding test accessibility.

```{r rit_excep_plots}
# code for plots showing the difference in average performance between groups
# as well as within-group variation (e.g., sina plots, raincloud plots, etc.)
```

## 4.3 Full Performance Continuum

The ORExt is designed to sample the Common Core State Standards in
English language arts (Reading, Writing, and Language) and Mathematics,
as well as the Oregon Science Standards and Next Generation Science
Standards in science in a purposive, validated manner. The ORExt test
blueprints convey the balance of representation exhibited by the
assessment (see *Appendix* 2.1B). These test blueprints are supported by
the [ORExt Extended Assessment
Frameworks](http://www.brtprojects.org/publications/training-modules),
which define the assessable content on the ORExt that has been reduced
in depth, breadth, and complexity (RDBC) using our defined process (see
*Appendix* 2.3A.3). The decisions regarding which standards to target
for essentialization, as well as the strength of linkage between the
Essentialized Standards and the CCSS/ORSci/NGSS has been validated by
Oregon teachers, as well (see *Appendix* 3.1A).

Though a simplified and standardized approach was taken to design items,
and efficiency and access to the assessment increased for the majority
of students (as evidenced by the decreased percentages of zero scores
across all content areas), a small subgroup of students remains who
cannot access an academic assessment. This is true even though items
have been significantly RDBC at three levels of complexity
(low-medium-high difficulty). As a response, ODE commissioned BRT to
design and implement an observational rating scale for this group of
very low-performing students, called the Oregon Observational Rating
Assessment (ORora) for the spring 2016 administration. The ORora targets
communication (expressive and receptive) and basic skills
(attention/joint attention and mathematics) and provides documentation
of student progress outside of our clearly defined academic domains.

Items on all assessments were scored on a 2-point scale, with 1 point
awarded for a correct response and 0 points awarded for an incorrect
response. Plots are provided below for each content area and grade
level, including the person ability and item difficulty distributions.
In general, the descriptive statistics suggest that the test had an
appropriate range of item difficulties represented, from easy to
difficult, with item difficulties generally ranging from -4.0 to +4.0 on
the Rasch scale. The assessments performed as expected across all grades
and content areas. The item person distributions provided below
demonstrate that the ORExt is providing a performance continuum for
students who participate.

### English Language Arts Person/Item Distributions

```{r }
# code for ELA person-item distributions
```

### Mathematics Person/Item Distributions

```{r }
# code for Math person-item distributions
```

### Science Person/Item Distributions

```{r }
# code for Science person-item distributions
```

## 4.4 Scoring

All scoring expectations for the ORExt are established within the
Administration Manual (see *Appendix* 2.3, p. 14). The scoring
procedures for the new ORExt have been simplified, with students
receiving a 0 for an incorrect response or a 1 for a correct response.
Input from the field gathered from Consequential Validity studies
demonstrates that the assessment scoring procedures are much more clear
and easier to implement than prior scoring approaches (see *Appendix*
2.3B.10). BRT was also commissioned to develop a scaled score
interpretation guide, which describes specific strategies for
interpreting student test scores and sub-test scores in Reading and
Writing, and Achievement Level Descriptors (ALDs) published within the
Individual Student Reports (see *Appendix* 6.4C) for annual performance,
growth, and as part of Essential Skills requirements for very low
performing students (see *Appendix* 2.1A).

## 4.5 Multiple Assessment Forms

The ORExt was administered in only form per subject area and grade level
for the 2017-18 school year, with 36 operational items arranged in order
of empirical difficulty and 12 embedded field test items.

## 4.6 Multiple Versions of An Assessment

The ORExt is provided in the standard format, but is also available in
Large Print and Brailled formats. Test content is identical across all
three versions, with an occasional item being eliminated on the Braille
version due to inaccessibility. These items do not count for or against
the student in reporting. Substantive test comparability analyses are
not feasible, given the small n-sizes of the samples involved in the
alternative versions.

## 4.7 Technical Analyses and Ongoing Maintenance

The ORExt technical analyses that document reliability and validity are
included in this technical report (see Sections 3 and 4, respectively).
ODE and BRT staff review these analyses annually. Necessary adjustments
to the assessment are determined prior to implementation of the
subsequent year's work plan, which elaborates the areas of improvement
as well as aspects of the testing program that will be maintained. This
decision-making is supported by input from the field gathered from the
Consequential Validity study (see *Appendix* 2.3B.10).

Within our system of ongoing improvement is continuation of the
development of additional curricular and instructional resources. This
addresses an area of concern expressed by stakeholders. Training modules
and templates continue to be developed to connect assessment results
from the ORExt and ORora with curricular resources and instructional
strategies aligned to the standards.
